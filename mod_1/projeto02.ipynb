{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto 02 - Árvores de Decisão\n",
    "**Estudante:** João Rodrigues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando recursos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando as funções com as árvores de decisão:\n",
    "Cada função também descreve o tipo de entrada e o tipo de saída.\n",
    "\n",
    "Note que eu decidi receber dentro das árvores um dicionário, essa escolha\n",
    "foi puramente baseada em achar mais fácil trabalhar com dicionários.\n",
    "\n",
    "Entretanto, cada dicionário deve conter apenas uma linha (row) do dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_decision_tree_one(dict: dict) -> float:\n",
    "  \"\"\"\n",
    "  Returns an float corresponding to the target species\n",
    "  0.0: Iris-setosa\n",
    "  1.0: Iris-versicolor\n",
    "  2.0: Iris-virginica\n",
    "\n",
    "    Arguments:\n",
    "      dict (dict): A dictionary containing exactly one row from a iris dataframe\n",
    "\n",
    "    Returns:\n",
    "      float: The target species\n",
    "  \"\"\"  \n",
    "  if not dict['petal length (cm)'] <= 2.45:\n",
    "    if dict['petal width (cm)'] <= 1.75:\n",
    "      if not dict['petal length (cm)'] <= 4.95:\n",
    "        if not dict['petal width (cm)'] <= 1.55:\n",
    "          return 1.0\n",
    "        \n",
    "        return 2.0\n",
    "\n",
    "      if not dict['petal width (cm)'] <= 1.65:\n",
    "        return 2.0\n",
    "      \n",
    "      return 1.0\n",
    "\n",
    "    if dict['petal length (cm)'] <= 4.85:\n",
    "      if dict['sepal width (cm)'] <= 3.1:\n",
    "        return 2.0\n",
    "      \n",
    "      return 1.0\n",
    "    \n",
    "    return 2.0\n",
    "\n",
    "  return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_decision_tree_two(dict: dict) -> float:\n",
    "  \"\"\"\n",
    "  Returns an float corresponding to the target species\n",
    "  0.0: Iris-setosa\n",
    "  1.0: Iris-versicolor\n",
    "  2.0: Iris-virginica\n",
    "\n",
    "    Arguments:\n",
    "      dict (dict): A dictionary containing exactly one row from a iris dataframe\n",
    "\n",
    "    Returns:\n",
    "      float: The target species\n",
    "  \"\"\"\n",
    "  if dict['petal length (cm)'] <= 2.45:\n",
    "    return 0.0\n",
    "  elif dict['petal width (cm)'] <= 1.75:\n",
    "    return 1.0\n",
    "  else:\n",
    "    return 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando as funções de avaliação do desempenho\n",
    "\n",
    "Com as árvores prontas, é hora de construir as funções que vão avaliar o desempenho.\n",
    "\n",
    "Para isso, decidi construir cada etapa de forma mais abstrata que consegui,\n",
    "podendo assim passar uma função como argumento de outra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(model_result, data: dict) -> tuple[(bool, float)]:\n",
    "  \"\"\"\n",
    "  Returns a tuple of (bool, float)\n",
    "  Bool: if the predicted species matches the actual species\n",
    "  Float: Corresponding for the predicted species\n",
    "\n",
    "      Arguments:\n",
    "        model_result (float): The predicted species\n",
    "        data (dict): A dictionary containing the data passed to the model\n",
    "\n",
    "      Returns:\n",
    "        tuple (bool, float): A tuple of (bool, float)\n",
    "        Bool: if the predicted species matches the actual species\n",
    "        Float: Corresponding for the predicted species\n",
    "  \n",
    "      Species:\n",
    "        0.0: Iris-setosa\n",
    "        1.0: Iris-versicolor\n",
    "        2.0: Iris-virginica\n",
    "  \"\"\"\n",
    "\n",
    "  return (data['target'] == model_result, model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(model: Callable, dataframe: pd.DataFrame) -> list[list[int]]:\n",
    "  \"\"\"\n",
    "  Returns a confusion matrix (list of lists)\n",
    "\n",
    "      Arguments:\n",
    "        model (function): A function that takes a dictionary as input and returns a float\n",
    "        dataframe (pandas.DataFrame): A dataframe containing the data to be used for the model\n",
    "      \n",
    "      Returns:\n",
    "        matrix list[list[int]]: A confusion matrix (list of lists)\n",
    "  \"\"\"\n",
    "\n",
    "  matrix = [[0,0,0],[0,0,0],[0,0,0]]\n",
    "  predicted_class0 = []\n",
    "  predicted_class1 = []\n",
    "  predicted_class2 = []\n",
    "\n",
    "  dataframe_to_dict = dataframe.filter(['target']).to_dict()\n",
    "\n",
    "  for key, value in dataframe_to_dict['target'].items():\n",
    "    if value == 0.0:\n",
    "      predicted_class0.append(check_answer(model(dataframe.iloc[key].to_dict()), dataframe.iloc[key].to_dict()))\n",
    "      matrix[0][0] = predicted_class0.count((True, 0.0))\n",
    "      matrix[0][1] = predicted_class0.count((False, 1.0))\n",
    "      matrix[0][2] = predicted_class0.count((False, 2.0))\n",
    "\n",
    "    if value == 1.0:\n",
    "      predicted_class1.append(check_answer(model(dataframe.iloc[key].to_dict()), dataframe.iloc[key].to_dict()))\n",
    "      matrix[1][0] = predicted_class1.count((False, 0.0))\n",
    "      matrix[1][1] = predicted_class1.count((True, 1.0))\n",
    "      matrix[1][2] = predicted_class1.count((False, 2.0))\n",
    "\n",
    "    if value == 2.0:\n",
    "      predicted_class2.append(check_answer(model(dataframe.iloc[key].to_dict()), dataframe.iloc[key].to_dict()))\n",
    "      matrix[2][0] = predicted_class2.count((False, 0.0))\n",
    "      matrix[2][1] = predicted_class2.count((False, 1.0))\n",
    "      matrix[2][2] = predicted_class2.count((True, 2.0))\n",
    "\n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix: list[list[int]]) -> float:\n",
    "  \"\"\"\n",
    "  Returns the accuracy of the model\n",
    "      \n",
    "      Arguments:\n",
    "          confusion_matrix (list): Confusion matrix (list of lists)\n",
    "      \n",
    "      Returns:\n",
    "          accuracy (float): Accuracy of the model\n",
    "  \n",
    "      Formula: accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "  \"\"\"\n",
    "\n",
    "  sum_correct = confusion_matrix[0][0] + confusion_matrix[1][1] + confusion_matrix[2][2]\n",
    "  sum_all = 0\n",
    "  for i in range(0,3):\n",
    "    for j in range(3):\n",
    "      sum_all += confusion_matrix[i][j]\n",
    "  return sum_correct/sum_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(matrix: list, class_index: int) -> float:\n",
    "  \"\"\"\n",
    "  Returns the precision of the model for the given class (iris species)\n",
    "\n",
    "      Arguments:\n",
    "          matrix (list): Confusion matrix (list of lists)\n",
    "          class_index (int): Index of the class (0, 1, 2)\n",
    "            0: Iris-setosa\n",
    "            1: Iris-versicolor\n",
    "            2: Iris-virginica\n",
    "      \n",
    "      Returns:\n",
    "          precision (float): Precision of the model for the given class\n",
    "          if the class is not in the supported list [0, 1, 2] returns -1\n",
    "  \n",
    "      Formula: precision = TP / (TP + FP)\n",
    "  \"\"\"\n",
    "  \n",
    "  if class_index not in [0, 1, 2]:\n",
    "    return -1\n",
    "  return (matrix[class_index][class_index]/(matrix[class_index][0] \n",
    "                + matrix[class_index][1] + matrix[class_index][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(matrix: list, class_index: int) -> float:\n",
    "  \"\"\"\n",
    "  Returns the recall of the model for the given class (iris species)\n",
    "\n",
    "      Arguments:\n",
    "          matrix (list): Confusion matrix (list of lists)\n",
    "          class_index (int): Index of the class (0, 1, 2)\n",
    "            0: Iris-setosa\n",
    "            1: Iris-versicolor\n",
    "            2: Iris-virginica\n",
    "      \n",
    "      Returns:\n",
    "          recall (float): Recall of the model for the given class\n",
    "  \n",
    "      Formula: recall = TP / (TP + FN)\n",
    "  \"\"\"\n",
    "  \n",
    "  return (\n",
    "          matrix[class_index][class_index]\n",
    "          /(matrix[0][class_index] \n",
    "          + matrix[1][class_index] \n",
    "          + matrix[2][class_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(matrix: list, class_index: int) -> float:\n",
    "  \"\"\"\n",
    "  Returns the f1 score of the model for the given class (iris species)\n",
    "  \n",
    "      Arguments:\n",
    "          matrix (list): Confusion matrix (list of lists)\n",
    "          class_index (int): Index of the class (0, 1, 2)\n",
    "            0: Iris-setosa\n",
    "            1: Iris-versicolor\n",
    "            2: Iris-virginica\n",
    "      \n",
    "      Returns:\n",
    "          f1_score (float): F1 score of the model for the given class\n",
    "      \n",
    "      Formula: f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "  \"\"\"\n",
    "  \n",
    "  return (\n",
    "    2* (precision(matrix, class_index) * recall(matrix, class_index)) \n",
    "    / (precision(matrix, class_index) + recall(matrix, class_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hardest_class(matrix: list[list[int]]) -> float:\n",
    "  \"\"\"\n",
    "  Returns the index of the hardest class (iris species)\n",
    "  \n",
    "      Arguments:\n",
    "          matrix (list): Confusion matrix (list of lists)\n",
    "      \n",
    "      Returns:\n",
    "          target (float): Index of the hardest class to define\n",
    "  \"\"\"\n",
    "  errors_setosa = matrix[0][1] + matrix[0][2]\n",
    "  errors_versicolor = matrix[1][0] + matrix[1][2]\n",
    "  errors_virginica = matrix[2][0] + matrix[2][1]\n",
    "\n",
    "  if errors_setosa > errors_versicolor and errors_setosa > errors_virginica:\n",
    "    return 0.0\n",
    "  if errors_versicolor > errors_setosa and errors_versicolor > errors_virginica:\n",
    "    return 1.0\n",
    "  if errors_virginica > errors_setosa and errors_virginica > errors_versicolor:\n",
    "    return 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_iris_names(target: float) -> str:\n",
    "  \"\"\"\n",
    "  Returns the name of the class (iris species)\n",
    "  \n",
    "      Arguments:\n",
    "          target (float): Index of the class (0, 1, 2)\n",
    "            0: Iris-setosa\n",
    "            1: Iris-versicolor\n",
    "            2: Iris-virginica\n",
    "      \n",
    "      Returns:\n",
    "          target_name (str): Name of the class (iris species)\n",
    "  \"\"\"\n",
    "  if target == 0.0:\n",
    "    return \"Iris-setosa\"\n",
    "  if target == 1.0:\n",
    "    return \"Iris-versicolor\"\n",
    "  if target == 2.0:\n",
    "    return \"Iris-virginica\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o dataset Iris\n",
    "Agora vamos carregar o dataset Iris que virá do SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "dados = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hora de testar!\n",
    "\n",
    "Por fim, vamos rodar o código e avaliar os dois modelos primeiramente com\n",
    "uma matriz de confusão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix Modelo 1:\n",
      "[[50, 0, 0], [0, 50, 0], [0, 1, 49]]\n",
      "Confusion Matrix Modelo 2:\n",
      "[[50, 0, 0], [0, 49, 1], [0, 5, 45]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## avaliando o modelo 1 por confusion matrix\n",
    "model1_matrix = confusion_matrix(iris_decision_tree_one, dados)\n",
    "model2_matrix = confusion_matrix(iris_decision_tree_two, dados)\n",
    "\n",
    "print(f'''\n",
    "Confusion Matrix Modelo 1:\n",
    "{model1_matrix}\n",
    "Confusion Matrix Modelo 2:\n",
    "{model2_matrix}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_hard = hardest_class(model1_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos as matrizes de confusão prontas, vamos avaliar os modelos\n",
    "por acurácia.\n",
    "\n",
    "Note que a partir daqui, todos os resultados serão arredondados até a segunda\n",
    "casa decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acurácia Modelo 1:\n",
      "0.99\n",
      "Acurácia Modelo 2:\n",
      "0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## avaliando os modelos 1 e 2 por acurácia\n",
    "model1_accuracy = accuracy(model1_matrix)\n",
    "model2_accuracy = accuracy(model2_matrix)\n",
    "\n",
    "print(f'''\n",
    "Acurácia Modelo 1:\n",
    "{model1_accuracy:.2}\n",
    "Acurácia Modelo 2:\n",
    "{model2_accuracy:.2}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos avaliar cada modelo com base em sua precisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precisão do Modelo 1:\n",
      "Iris Setosa | Iris Versicolor | Iris Virginica\n",
      "     1.0    |       1.0       | 0.98\n",
      "\n",
      "Precisão do Modelo 2:\n",
      "Iris Setosa | Iris Versicolor | Iris Virginica\n",
      "     1.0    |       0.98       | 0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##avaliando os modelos 1 e 2 por precisão\n",
    "model1_precision_0 = precision(model1_matrix, 0)\n",
    "model1_precision_1 = precision(model1_matrix, 1)\n",
    "model1_precision_2 = precision(model1_matrix, 2)\n",
    "\n",
    "model2_precision_0 = precision(model2_matrix, 0)\n",
    "model2_precision_1 = precision(model2_matrix, 1)\n",
    "model2_precision_2 = precision(model2_matrix, 2)\n",
    "\n",
    "print(f'''\n",
    "Precisão do Modelo 1:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model1_precision_0:.2}{' '*3} | {' '*6}{model1_precision_1:.2}{' '*6} | {model1_precision_2:.2}\n",
    "\n",
    "Precisão do Modelo 2:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model2_precision_0:.2}{' '*3} | {' '*6}{model2_precision_1:.2}{' '*6} | {model2_precision_2:.2}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aproveitando que já estamos aqui, por quê não calcular o recall de cada modelo \n",
    "para cada feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall do Modelo 1:\n",
      "Iris Setosa | Iris Versicolor | Iris Virginica\n",
      "     1.0    |       0.98       | 1.0\n",
      "\n",
      "Recall do Modelo 2:\n",
      "Iris Setosa | Iris Versicolor | Iris Virginica\n",
      "     1.0    |       0.91       | 0.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1_recall_0 = recall(model1_matrix, 0)\n",
    "model1_recall_1 = recall(model1_matrix, 1)\n",
    "model1_recall_2 = recall(model1_matrix, 2)\n",
    "\n",
    "model2_recall_0 = recall(model2_matrix, 0)\n",
    "model2_recall_1 = recall(model2_matrix, 1)\n",
    "model2_recall_2 = recall(model2_matrix, 2)\n",
    "\n",
    "print(f'''\n",
    "Recall do Modelo 1:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model1_recall_0:.2}{' '*3} | {' '*6}{model1_recall_1:.2}{' '*6} | {model1_recall_2:.2}\n",
    "\n",
    "Recall do Modelo 2:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model2_recall_0:.2}{' '*3} | {' '*6}{model2_recall_1:.2}{' '*6} | {model2_recall_2:.2}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E pra fechar com chave de ouro, vamos calcular o f1-score de cada modelo \n",
    "para cada feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1-Score do Modelo 1:\n",
      "Iris Setosa | Iris Versicolor | Iris Virginica\n",
      "     1.0    |       0.99       | 0.99\n",
      "\n",
      "F1-Score do Modelo 2:\n",
      "Iris Setosa | Iris Versicolor | Iris Virginica\n",
      "     1.0    |       0.94       | 0.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1_f1score_0 = f1_score(model1_matrix, 0)\n",
    "model1_f1score_1 = f1_score(model1_matrix, 1)\n",
    "model1_f1score_2 = f1_score(model1_matrix, 2)\n",
    "\n",
    "model2_f1score_0 = f1_score(model2_matrix, 0)\n",
    "model2_f1score_1 = f1_score(model2_matrix, 1)\n",
    "model2_f1score_2 = f1_score(model2_matrix, 2)\n",
    "\n",
    "print(f'''\n",
    "F1-Score do Modelo 1:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model1_f1score_0:.2}{' '*3} | {' '*6}{model1_f1score_1:.2}{' '*6} | {model1_f1score_2:.2}\n",
    "\n",
    "F1-Score do Modelo 2:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model2_f1score_0:.2}{' '*3} | {' '*6}{model2_f1score_1:.2}{' '*6} | {model2_f1score_2:.2}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Últimos detalhes\n",
    "\n",
    "Agora que vimos várias métricas sobre os modelos, vamos avaliar qual modelo\n",
    "possui melhor desempenho.\n",
    "\n",
    "Para isso, poderíamos escolher quaisquer métricas e calcular de forma global \n",
    "(também conhecida como acurácia) OU checar os valores individuais para cada classe.\n",
    "\n",
    "Como já temos a acurácia pronto, eu escolhi usá-la para definir qual o melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O melhor modelo com base em acurácia é: modelo 1\n"
     ]
    }
   ],
   "source": [
    "print ('O melhor modelo com base em acurácia é:', 'modelo 1' if model1_accuracy > model2_accuracy else 'modelo 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pudemos perceber, o modelo 1 é o modelo com maior acurácia nos testes.\n",
    "\n",
    "Algo legal desse _if ternário_ feito acima é que mesmo que a acurácia de ambos\n",
    "fosse idêntica, ele retornaria o modelo 2, isso é proposital uma vez que o\n",
    "modelo 2 é menor e requer menor poder computacional.\n",
    "\n",
    "Logo abaixo, nós vamos descobrir qual flor é mais difícil de ser identificada\n",
    "por cada modelo usando o f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A espécie mais difícil para o Modelo 1: Iris-virginica\n",
      "A espécie mais difícil para o Modelo 2: Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "print('A espécie mais difícil para o Modelo 1:', map_iris_names(hardest_class(model1_matrix)))\n",
    "print('A espécie mais difícil para o Modelo 2:', map_iris_names(hardest_class(model2_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências Bibliográficas\n",
    "\n",
    "[Como usar loc e iloc no pandas? - Matheus Budkewicz (horaDeCodar)](https://medium.com/horadecodar/data-science-tips-02-como-usar-loc-e-iloc-no-pandas-fab58e214d87)\n",
    "\n",
    "[Confusion Matrix for Your Multi-Class Machine Learning Model - \n",
    "Joydwip Mohajon (Towards Data Science)\n",
    "](https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826)\n",
    "\n",
    "[Qual a melhor métrica para avaliar os modelos de Machine Learning? -  Juliana Scudilio (Flai)](https://www.flai.com.br/juscudilio/qual-a-melhor-metrica-para-avaliar-os-modelos-de-machine-learning/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
