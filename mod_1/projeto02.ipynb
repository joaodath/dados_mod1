{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto 02 - Árvores de Decisão\n",
    "**Estudante:** João Rodrigues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando recursos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando as funções com as árvores de decisão:\n",
    "Cada função também descreve o tipo de entrada e o tipo de saída.\n",
    "\n",
    "Note que eu decidi receber dentro das árvores um dicionário, essa escolha\n",
    "foi puramente baseada em achar mais fácil trabalhar com dicionários.\n",
    "\n",
    "Entretanto, cada dicionário deve conter apenas uma linha (row) do dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_decision_tree_one(dict: dict) -> float:\n",
    "  \"\"\"\n",
    "  function iris_decision_tree_one\n",
    "  Expects a dictionary containing exactly one row from a iris dataframe as input\n",
    "  Returns an float corresponding to the target as output\n",
    "  0.0: Iris-setosa\n",
    "  1.0: Iris-versicolor\n",
    "  2.0: Iris-virginica\n",
    "  \"\"\"  \n",
    "  if not dict['petal length (cm)'] <= 2.45:\n",
    "    if dict['petal width (cm)'] <= 1.75:\n",
    "      if not dict['petal length (cm)'] <= 4.95:\n",
    "        if not dict['petal width (cm)'] <= 1.55:\n",
    "          return 1.0\n",
    "        \n",
    "        return 2.0\n",
    "\n",
    "      if not dict['petal width (cm)'] <= 1.65:\n",
    "        return 2.0\n",
    "      \n",
    "      return 1.0\n",
    "\n",
    "    if dict['petal length (cm)'] <= 4.85:\n",
    "      if dict['sepal width (cm)'] <= 3.1:\n",
    "        return 2.0\n",
    "      \n",
    "      return 1.0\n",
    "    \n",
    "    return 2.0\n",
    "\n",
    "  return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_decision_tree_two(dict: dict) -> float:\n",
    "  \"\"\"\n",
    "  function iris_decision_tree_one\n",
    "  Expects a dictionary containing exactly one row from a iris dataframe as input\n",
    "  Returns an float corresponding to the target as output\n",
    "  0.0: Iris-setosa\n",
    "  1.0: Iris-versicolor\n",
    "  2.0: Iris-virginica\n",
    "  \"\"\"\n",
    "  if dict['petal length (cm)'] <= 2.45:\n",
    "    return 0.0\n",
    "  elif dict['petal width (cm)'] <= 1.75:\n",
    "    return 1.0\n",
    "  else:\n",
    "    return 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando as funções de avaliação do desempenho\n",
    "\n",
    "Com as árvores prontas, é hora de construir as funções que vão avaliar o desempenho.\n",
    "\n",
    "Para isso, decidi construir cada etapa de forma mais abstrata que consegui,\n",
    "podendo assim passar uma função como argumento de outra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(model_result, data: dict) -> tuple[(bool, float)]:\n",
    "  \"\"\"\n",
    "  Returns a tuple of (bool, float)\n",
    "  Bool: if the predicted species matches the actual species\n",
    "  Float: Corresponding for the predicted species\n",
    "\n",
    "      Arguments:\n",
    "        model_result (float): The predicted species\n",
    "        data (dict): A dictionary containing the data passed to the model\n",
    "\n",
    "      Returns:\n",
    "        tuple (bool, float): A tuple of (bool, float)\n",
    "        Bool: if the predicted species matches the actual species\n",
    "        Float: Corresponding for the predicted species\n",
    "  \n",
    "      Species:\n",
    "        0.0: Iris-setosa\n",
    "        1.0: Iris-versicolor\n",
    "        2.0: Iris-virginica\n",
    "  \"\"\"\n",
    "  if model_result == 0.0:\n",
    "    return (data['target'] == 0.0, 0.0)\n",
    "  if model_result == 1.0:\n",
    "    return (data['target'] == 1.0, 1.0)\n",
    "  if model_result == 2.0:\n",
    "    return (data['target'] == 2.0, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(model: Callable, dataframe: pd.DataFrame) -> list[list[int]]:\n",
    "  \"\"\"\n",
    "  Returns a confusion matrix (list of lists)\n",
    "\n",
    "      Arguments:\n",
    "        model (function): A function that takes a dictionary as input and returns a float\n",
    "        dataframe (pandas.DataFrame): A dataframe containing the data to be used for the model\n",
    "      \n",
    "      Returns:\n",
    "        matrix list[list[int]]: A confusion matrix (list of lists)\n",
    "  \"\"\"\n",
    "\n",
    "  matrix = [[0,0,0],[0,0,0],[0,0,0]]\n",
    "  predicted_class0 = []\n",
    "  predicted_class1 = []\n",
    "  predicted_class2 = []\n",
    "\n",
    "  dataframe_to_dict = dataframe.filter(['target']).to_dict()\n",
    "\n",
    "  for key, value in dataframe_to_dict['target'].items():\n",
    "    if value == 0.0:\n",
    "      predicted_class0.append(check_answer(model(dataframe.iloc[key].to_dict()), dataframe.iloc[key].to_dict()))\n",
    "      matrix[0][0] = predicted_class0.count((True, 0.0))\n",
    "      matrix[0][1] = predicted_class0.count((False, 1.0))\n",
    "      matrix[0][2] = predicted_class0.count((False, 2.0))\n",
    "\n",
    "    if value == 1.0:\n",
    "      predicted_class1.append(check_answer(model(dataframe.iloc[key].to_dict()), dataframe.iloc[key].to_dict()))\n",
    "      matrix[1][0] = predicted_class1.count((False, 0.0))\n",
    "      matrix[1][1] = predicted_class1.count((True, 1.0))\n",
    "      matrix[1][2] = predicted_class1.count((False, 2.0))\n",
    "\n",
    "    if value == 2.0:\n",
    "      predicted_class2.append(check_answer(model(dataframe.iloc[key].to_dict()), dataframe.iloc[key].to_dict()))\n",
    "      matrix[2][0] = predicted_class2.count((False, 0.0))\n",
    "      matrix[2][1] = predicted_class2.count((False, 1.0))\n",
    "      matrix[2][2] = predicted_class2.count((True, 2.0))\n",
    "\n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix: list[list[int]]) -> float:\n",
    "  \"\"\"\n",
    "  Returns the accuracy of the model\n",
    "      \n",
    "      Arguments:\n",
    "          confusion_matrix (list): Confusion matrix (list of lists)\n",
    "      \n",
    "      Returns:\n",
    "          accuracy (float): Accuracy of the model\n",
    "  \n",
    "      Formula: accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "  \"\"\"\n",
    "\n",
    "  sum_correct = confusion_matrix[0][0] + confusion_matrix[1][1] + confusion_matrix[2][2]\n",
    "  sum_all = 0\n",
    "  for i in range(0,3):\n",
    "    for j in range(3):\n",
    "      sum_all += confusion_matrix[i][j]\n",
    "  return sum_correct/sum_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(matrix: list, class_index: int) -> float:\n",
    "  \"\"\"\n",
    "  Returns the precision of the model for the given class (iris species)\n",
    "\n",
    "      Arguments:\n",
    "          matrix (list): Confusion matrix (list of lists)\n",
    "          class_index (int): Index of the class (0, 1, 2)\n",
    "            0: Iris-setosa\n",
    "            1: Iris-versicolor\n",
    "            2: Iris-virginica\n",
    "      \n",
    "      Returns:\n",
    "          precision (float): Precision of the model for the given class\n",
    "          if the class is not in the supported list [0, 1, 2] returns -1\n",
    "  \n",
    "      Formula: precision = TP / (TP + FP)\n",
    "  \"\"\"\n",
    "  \n",
    "  if class_index not in [0, 1, 2]:\n",
    "    return -1\n",
    "  return (matrix[class_index][class_index]/(matrix[class_index][0] \n",
    "                + matrix[class_index][1] + matrix[class_index][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(matrix: list, class_index: int):\n",
    "  \"\"\"\n",
    "  Returns the recall of the model for the given class (iris species)\n",
    "\n",
    "      Arguments:\n",
    "          matrix (list): Confusion matrix (list of lists)\n",
    "          class_index (int): Index of the class (0, 1, 2)\n",
    "            0: Iris-setosa\n",
    "            1: Iris-versicolor\n",
    "            2: Iris-virginica\n",
    "      \n",
    "      Returns:\n",
    "          recall (float): Recall of the model for the given class\n",
    "  \n",
    "      Formula: recall = TP / (TP + FN)\n",
    "  \"\"\"\n",
    "  \n",
    "  return (\n",
    "          matrix[class_index][class_index]\n",
    "          /(matrix[0][class_index] \n",
    "          + matrix[1][class_index] \n",
    "          + matrix[2][class_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(matrix: list, class_index: int):\n",
    "  \"\"\"\n",
    "  Returns the f1 score of the model for the given class (iris species)\n",
    "  \n",
    "      Arguments:\n",
    "          matrix (list): Confusion matrix (list of lists)\n",
    "          class_index (int): Index of the class (0, 1, 2)\n",
    "            0: Iris-setosa\n",
    "            1: Iris-versicolor\n",
    "            2: Iris-virginica\n",
    "      \n",
    "      Returns:\n",
    "          f1_score (float): F1 score of the model for the given class\n",
    "      \n",
    "      Formula: f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "  \"\"\"\n",
    "  \n",
    "  return (\n",
    "    2* (precision(matrix, class_index) * recall(matrix, class_index)) \n",
    "    / (precision(matrix, class_index) + recall(matrix, class_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o dataset Iris\n",
    "Agora vamos carregar o dataset Iris que virá do SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "dados = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hora de testar!\n",
    "\n",
    "Por fim, vamos rodar o código e avaliar os dois modelos primeiramente com\n",
    "uma matriz de confusão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## avaliando o modelo 1 por confusion matrix\n",
    "model1_matrix = confusion_matrix(iris_decision_tree_one, dados)\n",
    "model2_matrix = confusion_matrix(iris_decision_tree_two, dados)\n",
    "\n",
    "print(f'''\n",
    "Confusion Matrix Modelo 1:\n",
    "{model1_matrix}\n",
    "Confusion Matrix Modelo 2:\n",
    "{model2_matrix}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos as matrizes de confusão prontas, vamos avaliar os modelos\n",
    "por acurácia.\n",
    "\n",
    "Note que a partir daqui, todos os resultados serão arredondados até a segunda\n",
    "casa decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## avaliando os modelos 1 e 2 por acurácia\n",
    "model1_accuracy = accuracy(model1_matrix)\n",
    "model2_accuracy = accuracy(model2_matrix)\n",
    "\n",
    "print(f'''\n",
    "Acurácia Modelo 1:\n",
    "{model1_accuracy:.2}\n",
    "Acurácia Modelo 2:\n",
    "{model2_accuracy:.2}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos avaliar cada modelo com base em sua precisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##avaliando os modelos 1 e 2 por precisão\n",
    "model1_precision_0 = precision(model1_matrix, 0)\n",
    "model1_precision_1 = precision(model1_matrix, 1)\n",
    "model1_precision_2 = precision(model1_matrix, 2)\n",
    "\n",
    "model2_precision_0 = precision(model2_matrix, 0)\n",
    "model2_precision_1 = precision(model2_matrix, 1)\n",
    "model2_precision_2 = precision(model2_matrix, 2)\n",
    "\n",
    "print(f'''\n",
    "Precisão do Modelo 1:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model1_precision_0:.2}{' '*3} | {' '*6}{model1_precision_1:.2}{' '*6} | {model1_precision_2:.2}\n",
    "\n",
    "Precisão do Modelo 2:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model2_precision_0:.2}{' '*3} | {' '*6}{model2_precision_1:.2}{' '*6} | {model2_precision_2:.2}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aproveitando que já estamos aqui, por quê não calcular o recall de cada modelo \n",
    "para cada feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_recall_0 = recall(model1_matrix, 0)\n",
    "model1_recall_1 = recall(model1_matrix, 1)\n",
    "model1_recall_2 = recall(model1_matrix, 2)\n",
    "\n",
    "model2_recall_0 = recall(model2_matrix, 0)\n",
    "model2_recall_1 = recall(model2_matrix, 1)\n",
    "model2_recall_2 = recall(model2_matrix, 2)\n",
    "\n",
    "print(f'''\n",
    "Recall do Modelo 1:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model1_recall_0:.2}{' '*3} | {' '*6}{model1_recall_1:.2}{' '*6} | {model1_recall_2:.2}\n",
    "\n",
    "Recall do Modelo 2:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model2_recall_0:.2}{' '*3} | {' '*6}{model2_recall_1:.2}{' '*6} | {model2_recall_2:.2}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E pra fechar com chave de ouro, vamos calcular o f1-score de cada modelo \n",
    "para cada feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_f1score_0 = f1_score(model1_matrix, 0)\n",
    "model1_f1score_1 = f1_score(model1_matrix, 1)\n",
    "model1_f1score_2 = f1_score(model1_matrix, 2)\n",
    "\n",
    "model2_f1score_0 = f1_score(model2_matrix, 0)\n",
    "model2_f1score_1 = f1_score(model2_matrix, 1)\n",
    "model2_f1score_2 = f1_score(model2_matrix, 2)\n",
    "\n",
    "print(f'''\n",
    "F1-Score do Modelo 1:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model1_f1score_0:.2}{' '*3} | {' '*6}{model1_f1score_1:.2}{' '*6} | {model1_f1score_2:.2}\n",
    "\n",
    "F1-Score do Modelo 2:\n",
    "Iris Setosa | Iris Versicolor | Iris Virginica\n",
    "{' '*5}{model2_f1score_0:.2}{' '*3} | {' '*6}{model2_f1score_1:.2}{' '*6} | {model2_f1score_2:.2}\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
