{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Processamento de Linguagem Natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m121.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 KB\u001b[0m \u001b[31m141.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.8/128.8 KB\u001b[0m \u001b[31m141.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Using cached pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 KB\u001b[0m \u001b[31m123.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m111.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 KB\u001b[0m \u001b[31m96.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy) (1.22.3)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 KB\u001b[0m \u001b[31m122.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.6/457.6 KB\u001b[0m \u001b[31m133.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.7-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: jinja2 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 KB\u001b[0m \u001b[31m176.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 KB\u001b[0m \u001b[31m91.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.5.18.1-py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.2/155.2 KB\u001b[0m \u001b[31m106.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting click<9.0.0,>=7.1.1\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 KB\u001b[0m \u001b[31m113.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: wasabi, murmurhash, cymem, urllib3, typing-extensions, tqdm, spacy-loggers, spacy-legacy, smart-open, preshed, langcodes, idna, click, charset-normalizer, certifi, catalogue, blis, typer, srsly, requests, pydantic, thinc, pathy, spacy\n",
      "Successfully installed blis-0.7.7 catalogue-2.0.7 certifi-2022.5.18.1 charset-normalizer-2.0.12 click-8.1.3 cymem-2.0.6 idna-3.3 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 requests-2.27.1 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.17 tqdm-4.64.0 typer-0.4.1 typing-extensions-4.2.0 urllib3-1.26.9 wasabi-0.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# instalando recursos necessários\n",
    "%pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.3.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.3.0/pt_core_news_sm-3.3.0-py3-none-any.whl (13.0 MB)\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from pt-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: setuptools in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (59.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (1.22.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: jinja2 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-sm==3.3.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "#carregando spacy\n",
    "import spacy\n",
    "spacy.cli.download('pt_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregando modelo do spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Para enxergar claro, bastar mudar a direção do olhar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para\n",
      "enxergar\n",
      "claro\n",
      ",\n",
      "bastar\n",
      "mudar\n",
      "a\n",
      "direção\n",
      "do\n",
      "olhar\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É chamado de span (não confundir com spam) um subconjunto de palavras do texto, para isso fazemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para enxergar claro\n"
     ]
    }
   ],
   "source": [
    "span = doc[0:3]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções is_alpha, is_punct e like_num verificam se o token é um texto, pontuação ou um número respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fluxos de processamento\n",
    "Os fluxos de processamento servem para identificar marcadores de classes gramaticais (identificar substantivos, verbos e adjetivos), dependências sintáticas e entidades nomeadas.\n",
    "O fluxo de processamento possui três tamanhos baseados na quantidade de conteúdo https://spacy.io/models/pt:\n",
    "\n",
    "- final _sm: small - pequeno (12 mb)\n",
    "\n",
    "- final _md: medium - médio (40 mb)\n",
    "\n",
    "- final _lg: large - grande (541 MB)\n",
    "\n",
    "Para realizar a instalação de um fluxo de processamento em português, use o respectivo comando abaixo (não é necessário instalar os três, caso queira somente testar, instale o primeiro pacote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spacy.cli.download('pt_core_news_sm')\n",
    "- spacy.cli.download('pt_core_news_md')\n",
    "- spacy.cli.download('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mais exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp('Eu comi uma deliciosa pizza')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu PRON\n",
      "comi VERB\n",
      "uma DET\n",
      "deliciosa ADJ\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Identificamos um pronome (PRON), um verbo (VERB), um artigo indefinido (DET), um adjetivo (ADJ) e um substantivo (NOUN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Brasil LOC\n",
      "R$ MISC\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp('Eu tenho usado o serviço de armazenamento na nuvem da Google, é a opção mais barata no Brasil, pago somente R$ 9.99 por mês.')\n",
    "\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Eu tenho usado o serviço de armazenamento na nuvem da \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", é a opção mais barata no \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Brasil\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", pago somente \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    R$\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " 9.99 por mês.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc2, style = \"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-lg==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.3.0/pt_core_news_lg-3.3.0-py3-none-any.whl (568.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 568.2/568.2 MB 482.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from pt-core-news-lg==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (1.22.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (59.6.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/joao/GitHub/Blue/trilha_dados/dados_mod1/.projeto2/lib/python3.10/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.1.1)\n",
      "Installing collected packages: pt-core-news-lg\n",
      "Successfully installed pt-core-news-lg-3.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download('pt_core_news_lg')\n",
    "nlp_lg = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Brasil LOC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Eu tenho usado o serviço de armazenamento na nuvem da \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", é a opção mais barata no \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Brasil\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", pago somente R$ 9.99 por mês.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc2_lg = nlp_lg('Eu tenho usado o serviço de armazenamento na nuvem da Google, é a opção mais barata no Brasil, pago somente R$ 9.99 por mês.')\n",
    "\n",
    "for ent in doc2_lg.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "displacy.render(doc2_lg, style = \"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Marcação do reconhecimento de entidades\n",
    "\n",
    "O uso de processamento de linguagem natural ainda não está tão maduro para a língua portuguesa como está para a língua inglesa. Observem que R$ não foi classificado como MONEY.\n",
    "\n",
    "Podemos também criar nossas entidades, para isso devemos criar um span com rótulo e adicionar a lista de entidades do doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá forasteiro GREETING\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "doc3 = nlp('Olá forasteiro, esta manhã teremos café, pão, ovos mexidos e manteiga, deseja que anotemos o pedido?')\n",
    "\n",
    "span_with_label = Span(doc3, 0, 2, label=\"GREETING\")\n",
    "doc3.ents = [span_with_label]\n",
    "\n",
    "for ent in doc3.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O spacy não possui um stemmer, havendo a necessidade de utilizar uma outra biblioteca muito famosa na área de processamento de linguagem natural: a nltk. \n",
    "\n",
    "Não iremos abordar sobre essa biblioteca, mas, para mais informações, vocês podem acessar o livro online gratuito.\n",
    "\n",
    "Por padrão, o spacy possui uma função lemma embutida ao objeto. Para usá-la, siga o comando abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisa precisar\n",
      "entender entender\n",
      "despertar despertar\n",
      "lutar lutar\n",
      "protegê-lo protegê-lo\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp('Você precisa entender, a maioria destas pessoas não está preparada para despertar. E muitas delas estão tão inertes, tão desesperadamente dependentes do sistema, que irão lutar para protegê-lo.')\n",
    "\n",
    "for token in doc4:\n",
    "    if token.pos_ == 'VERB':\n",
    "        print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra técnica muito utilizada é a verificação de similaridade. Com o spacy podemos checar se um documento ou um span ou um token são similares. Veremos nos exemplos abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9831460868659236\n",
      "0.8485166471843034\n"
     ]
    }
   ],
   "source": [
    "doc5 = nlp_lg('Eu gosto de pizza.')\n",
    "doc6 = nlp_lg('Eu gosto de fast-food.')\n",
    "doc7 = nlp_lg('Eu prefiro comer salada.')\n",
    "\n",
    "print(doc5.similarity(doc6))\n",
    "print(doc5.similarity(doc7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37072521448135376\n"
     ]
    }
   ],
   "source": [
    "doc8 = nlp_lg(\"Temos sorvete, pizza, batata-frita e refrigerante.\")\n",
    "token1 = doc8[3]\n",
    "token2 = doc8[7]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E como é feita a similaridade entre palavras? Utilizando vetores de palavras, cujo objetivo é permitir que palavras próximas estejam agrupadas mais próximas de si, por exemplo as palavras peixe e tubarão aparecem mais vezes juntas do que peixe e mamífero, logo peixe e tubarão estão mais próximas semanticamente.\n",
    "\n",
    "Um ponto a ser levado em consideração é que a similaridade depende do contexto da aplicação, observe o exemplo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968480081567788\n"
     ]
    }
   ],
   "source": [
    "doc9  = nlp_lg('Eu gosto cachorros.')\n",
    "doc10 = nlp_lg('Eu odeio cachorros.')\n",
    "\n",
    "print(doc9.similarity(doc10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraia do texto todas os substantivos e verbos do seguinte texto abaixo:\n",
    "\n",
    "CAPÍTULO PRIMEIRO / ÓBITO DO AUTOR\n",
    "Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo: diferença radical entre este livro e o Pentateuco.\n",
    "Dito isto, expirei às duas horas da tarde de uma sexta-feira do mês de agosto de 1869, na minha bela chácara de Catumbi. Tinha uns sessenta e quatro anos, rijos e prósperos, era solteiro, possuía cerca de trezentos contos e fui acompanhado ao cemitério por onze amigos. Onze amigos! Verdade é que não houve cartas nem anúncios. Acresce que chovia — peneirava uma chuvinha miúda, triste e constante, tão constante e tão triste, que levou um daqueles fiéis da última hora a intercalar esta engenhosa idéia no discurso que proferiu à beira de minha cova: — \"Vós, que o conhecestes, meus senhores, vós podeis dizer comigo que a natureza parece estar chorando a perda irreparável de um dos mais belos caracteres que têm honrado a humanidade. Este ar sombrio, estas gotas do céu, aquelas nuvens escuras que cobrem o azul como um crepe funéreo, tudo isso é a dor crua e má que lhe rói à Natureza as mais íntimas entranhas; tudo isso é um sublime louvor ao nosso ilustre finado.\"\n",
    "Bom e fiel amigo! Não, não me arrependo das vinte apólices que lhe deixei. E foi assim que cheguei à cláusula dos meus dias; foi assim que me encaminhei para o undiscovered country de Hamlet, sem as ânsias nem as dúvidas do moço príncipe, mas pausado e trôpego como quem se retira tarde do espetáculo. Tarde e aborrecido. Viram-me ir umas nove ou dez pessoas, entre elas três senhoras, minha irmã Sabina, casada com o Cotrim, a filha, — um lírio do vale, — e... Tenham paciência! daqui a pouco lhes direi quem era a terceira senhora. Contentem-se de saber que essa anônima, ainda que não parenta, padeceu mais do que as parentas. É verdade, padeceu mais. Não digo que se carpisse, não digo que se deixasse rolar pelo chão, convulsa. Nem o meu óbito era coisa altamente dramática... Um solteirão que expira aos sessenta e quatro anos, não parece que reúna em si todos os elementos de uma tragédia. E dado que sim, o que menos convinha a essa anônima era aparentá-lo. De pé, à cabeceira da cama, com os olhos estúpidos, a boca entreaberta, a triste senhora mal podia crer na minha extinção.\n",
    "— \"Morto! morto!\" dizia consigo.\n",
    "E a imaginação dela, como as cegonhas que um ilustre viajante viu desferirem o vôo desde o llisso às ribas africanas, sem embargo das ruínas e dos tempos, — a imaginação dessa senhora também voou por sobre os destroços presentes até às ribas de uma África juvenil... Deixá-la ir; lá iremos mais tarde; lá iremos quando eu me restituir aos primeiros anos. Agora, quero morrer tranqüilamente, metodicamente, ouvindo os soluços das damas, as falas baixas dos homens, a chuva que tamborila nas folhas de tinhorão da chácara, e o som estrídulo de uma navalha que um amolador está afiando lá fora, à porta de um correeiro. Juro-lhes que essa orquestra da morte foi muito menos triste do que podia parecer. De certo ponto em diante chegou a ser deliciosa. A vida estrebuchava- me no peito, com uns ímpetos de vaga marinha, esvaía-se-me a consciência, eu descia à imobilidade física e moral, e o corpo fazia-se-me planta, e pedra e lodo, e coisa nenhuma.\n",
    "Morri de uma pneumonia; mas se lhe disser que foi menos a pneumonia, do que uma idéia grandiosa e útil, a causa da minha morte, é possível que o leitor me não creia, e todavia é verdade. Vou expor-lhe sumariamente o caso. Julgue-o por si mesmo.\n",
    "\n",
    "------\n",
    "\n",
    "1. Quantos substantivos e verbos foram identificados?\n",
    "\n",
    "2. Separe todos os verbos em uma lista e aplique a Lemmatization sobre eles obtendo outra lista. Nesta nova lista remova as duplicatas e conte quantos verbos únicos há."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "brascubas = nlp_lg('''CAPÍTULO PRIMEIRO / ÓBITO DO AUTOR\n",
    "Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo: diferença radical entre este livro e o Pentateuco.\n",
    "Dito isto, expirei às duas horas da tarde de uma sexta-feira do mês de agosto de 1869, na minha bela chácara de Catumbi. Tinha uns sessenta e quatro anos, rijos e prósperos, era solteiro, possuía cerca de trezentos contos e fui acompanhado ao cemitério por onze amigos. Onze amigos! Verdade é que não houve cartas nem anúncios. Acresce que chovia — peneirava uma chuvinha miúda, triste e constante, tão constante e tão triste, que levou um daqueles fiéis da última hora a intercalar esta engenhosa idéia no discurso que proferiu à beira de minha cova: — \"Vós, que o conhecestes, meus senhores, vós podeis dizer comigo que a natureza parece estar chorando a perda irreparável de um dos mais belos caracteres que têm honrado a humanidade. Este ar sombrio, estas gotas do céu, aquelas nuvens escuras que cobrem o azul como um crepe funéreo, tudo isso é a dor crua e má que lhe rói à Natureza as mais íntimas entranhas; tudo isso é um sublime louvor ao nosso ilustre finado.\"\n",
    "Bom e fiel amigo! Não, não me arrependo das vinte apólices que lhe deixei. E foi assim que cheguei à cláusula dos meus dias; foi assim que me encaminhei para o undiscovered country de Hamlet, sem as ânsias nem as dúvidas do moço príncipe, mas pausado e trôpego como quem se retira tarde do espetáculo. Tarde e aborrecido. Viram-me ir umas nove ou dez pessoas, entre elas três senhoras, minha irmã Sabina, casada com o Cotrim, a filha, — um lírio do vale, — e... Tenham paciência! daqui a pouco lhes direi quem era a terceira senhora. Contentem-se de saber que essa anônima, ainda que não parenta, padeceu mais do que as parentas. É verdade, padeceu mais. Não digo que se carpisse, não digo que se deixasse rolar pelo chão, convulsa. Nem o meu óbito era coisa altamente dramática... Um solteirão que expira aos sessenta e quatro anos, não parece que reúna em si todos os elementos de uma tragédia. E dado que sim, o que menos convinha a essa anônima era aparentá-lo. De pé, à cabeceira da cama, com os olhos estúpidos, a boca entreaberta, a triste senhora mal podia crer na minha extinção.\n",
    "— \"Morto! morto!\" dizia consigo.\n",
    "E a imaginação dela, como as cegonhas que um ilustre viajante viu desferirem o vôo desde o llisso às ribas africanas, sem embargo das ruínas e dos tempos, — a imaginação dessa senhora também voou por sobre os destroços presentes até às ribas de uma África juvenil... Deixá-la ir; lá iremos mais tarde; lá iremos quando eu me restituir aos primeiros anos. Agora, quero morrer tranqüilamente, metodicamente, ouvindo os soluços das damas, as falas baixas dos homens, a chuva que tamborila nas folhas de tinhorão da chácara, e o som estrídulo de uma navalha que um amolador está afiando lá fora, à porta de um correeiro. Juro-lhes que essa orquestra da morte foi muito menos triste do que podia parecer. De certo ponto em diante chegou a ser deliciosa. A vida estrebuchava- me no peito, com uns ímpetos de vaga marinha, esvaía-se-me a consciência, eu descia à imobilidade física e moral, e o corpo fazia-se-me planta, e pedra e lodo, e coisa nenhuma.\n",
    "Morri de uma pneumonia; mas se lhe disser que foi menos a pneumonia, do que uma idéia grandiosa e útil, a causa da minha morte, é possível que o leitor me não creia, e todavia é verdade. Vou expor-lhe sumariamente o caso. Julgue-o por si mesmo.''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Quantos substantivos e verbos foram identificados?\n",
      "Verbos:  87\n",
      "Substantivos:  145\n"
     ]
    }
   ],
   "source": [
    "print('1. Quantos substantivos e verbos foram identificados?')\n",
    "\n",
    "brascubas_arr_verb = [{token.pos_: token.text} for token in brascubas if token.pos_ == 'VERB']\n",
    "brascubas_arr_verb = [(brascubas_arr_verb[key].values()) for key, value in enumerate(brascubas_arr_verb)]\n",
    "print('Verbos: ', len(brascubas_arr_verb))\n",
    "\n",
    "brascubas_arr_noun = [{token.pos_: token.text} for token in brascubas if token.pos_ == 'NOUN']\n",
    "brascubas_arr_noun = [brascubas_arr_noun[key].values() for key, value in enumerate(brascubas_arr_noun)]\n",
    "print('Substantivos: ', len(brascubas_arr_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Separe todos os verbos em uma lista e aplique a Lemmatization sobre eles obtendo outra lista. Nesta nova lista remova as duplicatas e conte quantos verbos únicos há."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_list(array: list) -> list:\n",
    "  token_list = []\n",
    "  for token in array:\n",
    "    token_list.append(*token)\n",
    "  return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(token_list: list, isset: bool) -> list:\n",
    "  \"\"\"\n",
    "  Expects a list of tokens/strings\n",
    "  Returns either a list with all lemmas (isset: False) or a list with unique lemmas (isset: True)\n",
    "  \"\"\"\n",
    "  lemma_list = []\n",
    "  for key in range(len(token_list)):\n",
    "    for token in nlp_lg(str(token_list[key])):\n",
    "      lemma_list.append(token.lemma_)\n",
    "  if isset:\n",
    "    return list(set(lemma_list))\n",
    "  return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "verb_list = get_token_list(brascubas_arr_verb)\n",
    "\n",
    "verb_lemma = get_lemma(verb_list, isset=True)\n",
    "\n",
    "print(len(verb_lemma))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c85a8c794e2ef35fe4da22a5230fa839f732614150129be0b83f468abb08431"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.projeto2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
